---
title: "Matching-Adjusted Indirect Comparison: Example using the MAIC package"
author: "BresMed and Roche"
date: "`r Sys.Date()`"
output: 
  html_vignette:
    number_sections: true
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{Matching-Adjusted Indirect Comparison: Example using the MAIC package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction {-}

This document describes the steps required to perform a matching-adjusted
indirect comparison (MAIC) analysis using the 'MAIC' package in R for a
disconnected treatment network where the endpoint of interest is either
time-to-event (e.g. overall survival) or binary (e.g. objective tumor
response).

The methods described in this document are based on those described by
both Signorovitch et al. 2012 and the National Institute for Health and Care
Excellence (NICE) Decision Support Unit (DSU) Technical Support Document (TSD)
18.[@signorovitch2012matching; @Phillippo2016a]

The premise of MAIC methods is to adjust for between-trial differences in
baseline patient characteristics. When a common treatment comparator or ‘linked
network’ are unavailable, a MAIC assumes that differences between absolute
outcomes that would be observed in each trial are entirely explained by
imbalances in prognostic variables and treatment effect modifiers. Under this
assumption, every prognostic variable and every treatment effect modifier that
is imbalanced between the two studies must be available. This assumption is
generally considered very difficult to meet.[@Phillippo2016a]
There are several ways of identifying prognostic variables/treatment effect
modifiers to be used in the MAIC analyses, some of which include:

* Clinical expertise (when available to a project)
* Published papers/previous submissions (what has been identified in the disease area previously)
* Exploratory analyses that look at univariable/multivariable analyses (regression models)
* Analyzing subgroup analyses presented in the clinical study report

The data used to create this document has been simulated to resemble that of
clinical trial data (in ADaM format). For the purpose of this example the
treatments being compared are 'intervention' and 'comparator', where individual
patient data is available for 'intervention' and aggregate summary data is
available for 'comparator'. In this example scenario, age, sex, ECOG PS and
smoking status have been identified as imbalanced prognostic variables/treatment
effect modifiers.


\


# Set up packages, directories and data

## Install packages

The following packages are required to run this example:

```{r, warning = FALSE, message = FALSE}
library(plyr)
library(dplyr)
library(boot)
library(survival)
library(MAIC)
```

## Set up directories

Define the following directories:

* The base directory (the folder that includes all relevant data and where the results will be saved)
* The data directory (where the data is saved)

```{r}
base_dir <- 'G:/Clients/Roche/2797 Development of R Code for MAIC and mixture cure models/Project'
data_path <- file.path(base_dir,'2 Exploratory/Simulated datasets')
```

## Read in the data

To perform anchored MAICs, the following data is required:

* Individual patient data (IPD) from the intervention trial
* Pseudo data for the comparator trial
* Baseline data from the comparator trial

### Intervention trial IPD
This example reads in data from three standard ADaM data sets (adsl, adrs and
adtte) which are saved as '.csv' files. The data needs some manipulation to
standardize the variable names and ensure the data will work in the MAIC
functions and create on data frame with all the necessary data for the
intervention treatment.

The survival variables should include:

* Time - a numeric value
* Event - a binary variable (event=1, censor=0)
* Treatment 

The binary variables should be called:

* Binary_event - a binary variable (event=1, no event=0)
* Treatment

All binary covariates to be used in the matching should be coded 1 and 0 (see
example for sex below).

```{r}
#### Intervention data

# Read in ADaM data and rename variables of interest
adsl <- read.csv(file.path(data_path, "adsl.csv")) %>% # subject level data
  dplyr::mutate(SEX=ifelse(SEX=="Male",1,0))

adrs <- read.csv(file.path(data_path, "adrs.csv")) %>% # response data
  dplyr::filter(PARAM=="Response") %>%
  dplyr::select(USUBJID, ARM, Binary_event=AVAL) ## change "Binary_event" to "response"?

adtte <- read.csv(file.path(data_path, "adtte.csv")) %>% # time to event data
  dplyr::filter(PARAMCD=="OS") %>%
  dplyr::mutate(Event=1-CNSR) %>%
  dplyr::select(USUBJID, ARM, Time=AVAL, Event)

# Combine all intervention data
intervention_input <- plyr::join_all(list(adsl, adrs, adtte), type = "full", by=c("USUBJID", "ARM"))
head(intervention_input)
```


### Comparator data

Naming of variables in the comparator data should be consistent with those used
in the intervention IPD.

It is common for response endpoints to be reported as a percentage of responders
and therefore the example code below simulates pseudo binary response data based
on the total number of patients and the proportion of responders.
```{r}

#### Comparator pseudo data

# read in digitised pseudo survival data
comparator_surv <- read.csv(file.path(data_path,"psuedo_IPD.csv"))

# simulate response data based on the known proportion of responders
comparator_n <- nrow(comparator_surv) # total number of patients in the comparator data
comparator_prop_events <- 0.4 # proportion of responders
comparator_binary <- data.frame("Binary_event"=
                                  c(rep(1,comparator_n*comparator_prop_events),
                                    rep(0, comparator_n*(1-comparator_prop_events))))

# join survival and response comparator data note not a 1:1 relationship - the
# rows do not represent the same observation
comparator_input <- cbind(comparator_surv, comparator_binary)

# Baseline aggregate data for the comparator population
target_pop <- read.csv(file.path(data_path,"Aggregate data.csv"))
target_pop


```

\


# Estimate weights

## Statistical theory 

As described by Signorovitch et al. (supplemental appendix), we must find a
$\beta$, such that re-weighting baseline characteristics, $x_{i,ild}$, by
$\hat{\omega}_i=\exp{(x_{i,ild}.\beta)}$, exactly matches the mean baseline
characteristics for the data source for which only aggregate data is available.
That is, we must find a solution to: $$ \bar{x}_{agg}\sum_{i=1}^n
\exp{(x_{i,ild}.\beta)}  = \sum_{i=1}^n x_{i,ild}.\exp{(x_{i,ild}.\beta)}\qquad
(1)  $$ This estimator is equivalent to solving the equation $$ 0 = \sum_{i=1}^n
(x_{i,ild} -  \bar{x}_{agg} ).\exp{(x_{i,ild}.\beta)}$$ without loss of
generality, it can be assumed that  $\bar{x}_{agg} = 0$ (e.g we could transform
baseline characteristics in both trials by subtracting  $\bar{x}_{agg})$
leaving the estimator $$0 = \sum_{i=1}^n (x_{i,ild})\exp{(x_{i,ild}.\beta)}.$$
The right hand side of this estimator is the first derivative of $$ Q(\beta) =
\sum_{i=1}^n  \exp{(x_{i,ild}.\beta)} $$ As described by Signorovitch et al
(supplemental appendix), $Q(\beta)$ is convex and therefore any finite solution
to (1) is unique and corresponds to the global minimum of $Q(\beta)$.


## Centering of baseline characteristics

As described above, in order to facilitate estimation of patient weights,
$\hat{\omega}_i$, it is necessary to center the baseline characteristics of the
intervention data using the mean baseline characteristics from the comparator
data.
Note, as described by Phillippo, it is necessary to balance on both mean and
standard deviation for continuous variables (where possible).
The code below also specifies two variables (cent_match_cov and match_cov) that
contain the names of the centered and non-centered matching variables - these
will be needed for the analyses later on.

```{r}
#### center baseline characteristics
# (subtract the aggregate comparator data from the corresponding column of intervention PLD)
names(intervention_input)
intervention_data <- intervention_input %>%
                          dplyr::mutate(Age_centered = AGE - target_pop$age.mean,
                                 # it is necessary to balance on both mean and standard deviation for continous variables:
                                 Age_squared_centered = (AGE^2) - (target_pop$age.mean^2 + target_pop$age.sd^2),
                                 Sex_centered = SEX - target_pop$prop.male,
                                 Smoke_centered = SMOKE - target_pop$prop.smoke,
                                 ECOG0_centered = ECOG0 - target_pop$prop.ecog0)
head(intervention_data)

# Set matching covariates
cent_match_cov <- c("Age_centered",
                    "Age_squared_centered",
                    "Sex_centered",
                    "Smoke_centered",
                    "ECOG0_centered")


match_cov <- c("AGE",
               "SEX",
               "SMOKE",
               "ECOG0")

# Renames target population cols to match match_cov
match_cov
names(target_pop)
target_pop_standard <- target_pop %>%
  #EDIT
  dplyr::rename(N=N,
                Treatment="ARM",
                AGE=age.mean,
                SEX=prop.male,
                SMOKE=prop.smoke,
                ECOG0=prop.ecog0
  ) %>%
  dplyr::select(N, Treatment, match_cov)


```


## Optimization procedure

Following centering of the baseline characteristics of the intervention study,
the optimization procedure can be performed to minimize $Q(\beta) =
\sum_{i=1}^n \exp{(x_{i,ild}.\beta)}$ and weights estimated using the
estimate_weights function in the MAIC package. This function outputs:

* An analysis data frame of combined intervention data and comparator data with
weights (patients in the pseudo comparator data are assigned weights of 1)
* A character vector containing the matching variables

```{r}
est_weights <- estimate_weights(intervention_data=intervention_data,
                                comparator_data=comparator_input,
                                matching_vars = cent_match_cov)

head(est_weights$analysis_data)
est_weights$matching_vars


```


## Weight diagnostics

Following the calculation of weights, it is necessary to determine whether the
optimization procedure has worked correctly and whether the weights derived are
sensible.




### Are the weights sensible?

#### Effective sample size

For a weighted estimate, the effective sample size (ESS) is the number of
independent non-weighted individuals that would be required to give an estimate
with the same precision as the weighted sample estimate. The approximate
effective sample size is calculated as: $$  \frac{({ \sum_{i=1}^n \hat{\omega}_i
})^2}{ \sum_{i=1}^n \hat{\omega^2}_i  } $$
A small ESS, relative to the original sample size, is an indication that the
weights are highly variable due to a lack of population overlap, and that the
estimate may be unstable.

The MAIC package includes a function to estimate the ESS, as well as produce a
summary of the weights and the profiles of patients with those weights.

```{r}
# Function to produce a set of diagnostics.
# Calls each of the diagnostic functions above except for plotting histograms
diagnostics <- filter(est_weights$analysis_data, ARM == 'Intervention') %>%
  wt_diagnostics(vars = est_weights$matching_vars)

(ESS <- diagnostics$ESS)
diagnostics$Summary_of_weights
diagnostics$Weight_profiles


```


#### Rescaled weights

It is easier to examine the distribution of the weights by scaling them, so that
the rescaled weights are relative to the original unit weights of each
individual; in other words, a rescaled weight $>$ 1 means that an individual
carries more weight in the re-weighted population than the original data. The
rescaled weight are calculated as:

$$\tilde{\omega}_i  =  \frac{  \hat{\omega}_i}{ \sum_{i=1}^n \hat{\omega}_i }.N $$

A histogram of the rescaled weights (along with a histogram of the weights) can
be produced using the `hist_wts()` function in the MAIC package.'bin_width'
needs to be adapted depending on the sample size in the data set

```{r}
# Plot histograms of unscaled and rescaled weights
# bin_width needs to be adapted depending on the sample size in the data set
histogram <- filter(est_weights$analysis_data, ARM == 'Intervention') %>%
  hist_wts(bin = 50)
histogram

```

### Has the optimization worked?

The first step is to check whether the re-weighted baseline characteristics for
the intervention-treated patients match those aggregate characteristics from the
comparator trial.

```{r}
# Set weighted and unweighted intervention data
baseline_analysis_data <- est_weights$analysis_data %>% filter(ARM=="Intervention") %>% dplyr::mutate(Treatment=paste0(ARM, "_matched")) %>%
  rbind(est_weights$analysis_data %>% filter(ARM=="Intervention")%>% mutate(Treatment=paste0(ARM, "_unadjusted"), wt = 1, wt_rs = 1)) %>%
  select(Treatment, match_cov, wt, wt_rs)

# Summerises the baseline characteristics
Baseline_summary <- baseline_analysis_data %>%
  dplyr::group_by(Treatment) %>%
  dplyr::summarise_each(list(~ weighted.mean(., wt)),-c(wt,wt_rs)) %>%
  rbind(target_pop_standard  %>% select(Treatment, match_cov) )

Baseline_summary_n <- baseline_analysis_data %>%
  dplyr::group_by(Treatment) %>%
  dplyr::summarise(
    'N' = n()) %>%
  rbind(target_pop_standard  %>% select(N, Treatment)) %>%
  dplyr::rename(`N/ESS`=N)


Baseline_summary_all <- dplyr::full_join(Baseline_summary_n, Baseline_summary, by="Treatment")

Baseline_summary <- cbind(Baseline_summary_all %>% select(-c(match_cov)),
                          lapply(Baseline_summary_all %>% select(match_cov), sprintf, fmt = "%.2f") %>% as.data.frame())


# replace N with ESS
Baseline_summary$`N/ESS`[Baseline_summary$Treatment == "Intervention_unadjusted"] <- ESS
Baseline_summary

```



# Incorporation of the weights in statistical analysis

## Bootstrapping a confidence interval
The use of weights induces a within-subject correlation in outcomes, as
observations can have weights that are unequal to one another
[@Austin2016; @Thernau2015]. As such, it is necessary to use a
variance estimator to take into account the lack of independence of
observations. The two common approaches to this are robust variance estimation
and bootstrapping. A simulation study was conducted by Austin et al
\cite{Austin2016} to examine the different methods in the context of an inverse
probability of treatment weighting (IPTW) survival analysis.  The author
concluded that the use of a bootstrap estimator resulted in approximately
correct estimates of standard errors and confidence intervals with the correct
coverage rate. The other estimators resulted in biased estimates of standard
errors and confidence intervals with incorrect coverage rates. The use of a
bootstrap type estimator is also intuitively appealing, a robust estimator
assumes that the weights are known and not subject to any sampling uncertainty.
However, a bootstrap estimator allows for quantification of the uncertainty in
the estimation of the weights. 

Bootstrapping involves:

1. Sampling, with replacement, from the patients in the
intervention arm (a bootstrap sample) 
2. Estimating a set of weights for each of
these bootstrapped data sets and 
3. Estimating a HR/OR using each set of estimated
weights. 

This procedure is repeated multiple times to obtain a distribution of HRs/ORs
for which the  2.5th and 97.5th percentile is used to generate the limits of a
95% confidence interval.

### Example for HRs
```{r}
# Demonstrate functionality of the bootstrap_HR function
# This function returns a single estimate of the hazard ratio
# This function is intended to be used in conjunction with the boot function, not called directly by the user
int <- filter(est_weights$analysis_data, ARM == 'Intervention')
comp <- filter(est_weights$analysis_data, ARM == 'Comparator')
HR <- bootstrap_HR(intervention_data=int, comparator_data=comp, matching = est_weights$matching_vars,
                  i=c(1:nrow(intervention_data)), model = Surv(Time, Event==1) ~ ARM
                  )

# Bootstrap estimates
HR_bootstraps <- boot(data = int, statistic = bootstrap_HR, R=1000, comparator_data=comp,
                      matching = est_weights$matching_vars, model = Surv(Time, Event==1) ~ ARM
                      )

# Summarise bootstrap estimates
hist(HR_bootstraps$t, main = "", xlab = "Boostrapped HR")
abline(v= quantile(HR_bootstraps$t, probs = c(0.025, 0.5, 0.975)), lty=2)

HR_median <- median(HR_bootstraps$t)
# Bootstrap CI function - Normal CI
boot_ci_HR <- boot.ci(boot.out = HR_bootstraps, index=1, type="norm") # takes specific values

# Bootstrap CI function - BCA CI
boot_ci_HR_BCA <- boot.ci(boot.out = HR_bootstraps, index=1, type="bca")


## HRs
unweighted_cox <- coxph(Surv(Time, Event==1) ~ ARM, data = est_weights$analysis_data)
weighted_cox <- coxph(Surv(Time, Event==1) ~ ARM, data = est_weights$analysis_data, weights = wt)


HR_summ <- rbind(summary(unweighted_cox)$conf.int, summary(weighted_cox)$conf.int) %>%
  as.data.frame() %>%
  dplyr::select(-`exp(-coef)`) %>% #drop unnecessary column
  dplyr::rename(HR = `exp(coef)`, HR_low_CI = `lower .95`, HR_upp_CI = `upper .95`) %>%
  mutate(Method = c("Unadjusted", "Cox weighted")) %>%
  rbind(data.frame("HR" = HR_median, "HR_low_CI" = boot_ci_HR$normal[2], "HR_upp_CI" = boot_ci_HR$normal[3], "Method"="Normal bootstrap")) %>%
  rbind(data.frame("HR" = HR_median, "HR_low_CI" = boot_ci_HR_BCA$bca[4], "HR_upp_CI" = boot_ci_HR_BCA$bca[5], "Method"="BCA bootstrap")) %>%
  dplyr::mutate(HR_95_CI = paste0(sprintf('%.3f', HR), " (", sprintf('%.3f', HR_low_CI), ", ", sprintf('%.3f', HR_upp_CI), ")")) %>%
  dplyr::select(Method, HR_95_CI)
HR_summ


```
### Example for ORs
```{r}

# Demonstrate functionality of the bootstrap_OR function
# This function returns a single estimate of the odds ratio
# This function is intended to be used in conjunction with the boot function, not called directly by the user
int <- filter(est_weights$analysis_data, ARM == 'Intervention')
comp <- filter(est_weights$analysis_data, ARM == 'Comparator')
OR <- bootstrap_OR(intervention_data=int, comparator_data=comp, matching = est_weights$matching_vars,
                  i=c(1:nrow(intervention_data)), model = 'Binary_event ~ ARM')




# Bootstrap estimates
OR_bootstraps <- boot(data = int, statistic = bootstrap_OR, R=1000, comparator_data=comp,
                      matching = est_weights$matching_vars, model = 'Binary_event ~ ARM'
                      )

# summarise bootstrap estimates
hist(OR_bootstraps$t, main = "", xlab = "Boostrapped OR")
abline(v= quantile(OR_bootstraps$t, probs = c(0.025,0.5,0.975)), lty=2)

OR_median <- median(OR_bootstraps$t)

# Bootstrap CI function - Normal CI
boot_ci_OR <- boot.ci(boot.out = OR_bootstraps, index=1, type="norm") # takes specific values

# Bootstrap CI function - BCA CI
boot_ci_OR_BCA <- boot.ci(boot.out = OR_bootstraps, index=1, type="bca")

# ORs
unweighted_OR <- suppressWarnings(glm(Binary_event~ARM, family=binomial(link="logit"), data = est_weights$analysis_data))
weighted_OR <- suppressWarnings(glm(Binary_event~ARM, family=binomial(link="logit"), data = est_weights$analysis_data, weight = wt))


OR_summ <- rbind(exp(cbind("Odds ratio" = coef(unweighted_OR), confint.default(unweighted_OR, level = 0.95)))[2,],
                 exp(cbind("Odds ratio" = coef(weighted_OR), confint.default(weighted_OR, level = 0.95)))[2,]) %>%
  as.data.frame() %>%
  dplyr::rename(OR = `Odds ratio`, OR_low_CI = `2.5 %`, OR_upp_CI = `97.5 %`) %>%
  mutate(Method = c("Unadjusted", "Weighted")) %>%
  rbind(data.frame("OR" = OR_median, "OR_low_CI" = boot_ci_OR$normal[2], "OR_upp_CI" = boot_ci_OR$normal[3], "Method"="Normal bootstrap")) %>%
  rbind(data.frame("OR" = OR_median, "OR_low_CI" = boot_ci_OR_BCA$bca[4], "OR_upp_CI" = boot_ci_OR_BCA$bca[5], "Method"="BCA bootstrap")) %>%
  dplyr::mutate(OR_95_CI = paste0(sprintf('%.3f', OR), " (", sprintf('%.3f', OR_low_CI), ", ", sprintf('%.3f', OR_upp_CI), ")")) %>%
  dplyr::select(Method, OR_95_CI)
OR_summ


```




# References
